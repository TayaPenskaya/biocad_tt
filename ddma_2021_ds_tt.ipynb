{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестовое задание на позицию Data Scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Science \\ Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимо построить модель, которая на основании поступающих каждую минуту данных определяет качество продукции, производимое на единице оборудования.\n",
    "\n",
    "Единица оборудования состоит из 5 одинаковых по размеру камер, в каждой камере установлено по 3 датчика температур. Также имеются данные о высоте слоя сырья и его влажности. Высота слоя и влажность измеряются при входе сырья в машину. Сырье проходит через единицу оборудования за 1 час.\n",
    "\n",
    "Данные с показателями работы оборудования содержатся в файле x_train.csv:\n",
    "\n",
    "| Название тега  | Описание тега |\n",
    "| :- | -: |\n",
    "| T_data_1_1\t| 1-й датчик в 1-й камере |\n",
    "| T_data_1_2\t| 2-й датчик в 1-й камере |\n",
    "| T_data_1_3\t| 3-й датчик в 1-й камере |\n",
    "| T_data_2_1\t| 1-й датчик во 2-й камере |\n",
    "| T_data_2_2\t| 2-й датчик во 2-й камере |\n",
    "| T_data_2_3\t| 3-й датчик во 2-й камере |\n",
    "| T_data_3_1\t| 1-й датчик в 3-й камере |\n",
    "| T_data_3_2\t| 2-й датчик в 3-й камере |\n",
    "| T_data_3_3\t| 3-й датчик в 3-й камере |\n",
    "| T_data_4_1\t| 1-й датчик в 4-й камере |\n",
    "| T_data_4_2\t| 2-й датчик в 4-й камере |\n",
    "| T_data_4_3\t| 3-й датчик в 4-й камере |\n",
    "| T_data_5_1\t| 1-й датчик в 5-й камере |\n",
    "| T_data_5_2\t| 2-й датчик в 5-й камере |\n",
    "| T_data_5_3\t| 3-й датчик в 5-й камере |\n",
    "| H_data\t| Высота слоя |\n",
    "| AH_data\t| Влажность сырья |\n",
    "\n",
    "\n",
    "Качество продукции измеряется в лаборатории по пробам, которые забираются каждый час, данные по известным анализам содержатся в файле y_train.csv. В файле указано время забора пробы, проба забирается на выходе из единицы оборудования.\n",
    "\n",
    "Вы договорились с заказчиком, что оценкой модели будет являться показатель MAE. \n",
    "\n",
    "Для оценки модели необходимо сгенерировать предсказания за период, указанный в файле x_test.csv (1184 предикта).\n",
    "\n",
    "Результатом является воспроизводимый Python код в данной тетрадке + приложенный csv файл с предсказаниями на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from math import trunc\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pd.read_csv('x_train.csv')\n",
    "y_train = pd.read_csv('y_train.csv')\n",
    "x_test = pd.read_csv('x_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>T_data_1_1</th>\n",
       "      <th>T_data_1_2</th>\n",
       "      <th>T_data_1_3</th>\n",
       "      <th>T_data_2_1</th>\n",
       "      <th>T_data_2_2</th>\n",
       "      <th>T_data_2_3</th>\n",
       "      <th>T_data_3_1</th>\n",
       "      <th>T_data_3_2</th>\n",
       "      <th>T_data_3_3</th>\n",
       "      <th>T_data_4_1</th>\n",
       "      <th>T_data_4_2</th>\n",
       "      <th>T_data_4_3</th>\n",
       "      <th>T_data_5_1</th>\n",
       "      <th>T_data_5_2</th>\n",
       "      <th>T_data_5_3</th>\n",
       "      <th>H_data</th>\n",
       "      <th>AH_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>212</td>\n",
       "      <td>210</td>\n",
       "      <td>211</td>\n",
       "      <td>347</td>\n",
       "      <td>353</td>\n",
       "      <td>347</td>\n",
       "      <td>474</td>\n",
       "      <td>473</td>\n",
       "      <td>481</td>\n",
       "      <td>346</td>\n",
       "      <td>348</td>\n",
       "      <td>355</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "      <td>167.85</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-01 00:01:00</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>346</td>\n",
       "      <td>352</td>\n",
       "      <td>346</td>\n",
       "      <td>475</td>\n",
       "      <td>473</td>\n",
       "      <td>481</td>\n",
       "      <td>349</td>\n",
       "      <td>348</td>\n",
       "      <td>355</td>\n",
       "      <td>241</td>\n",
       "      <td>241</td>\n",
       "      <td>243</td>\n",
       "      <td>162.51</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-01 00:02:00</td>\n",
       "      <td>212</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>345</td>\n",
       "      <td>352</td>\n",
       "      <td>346</td>\n",
       "      <td>476</td>\n",
       "      <td>473</td>\n",
       "      <td>481</td>\n",
       "      <td>352</td>\n",
       "      <td>349</td>\n",
       "      <td>355</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>242</td>\n",
       "      <td>164.99</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-01 00:03:00</td>\n",
       "      <td>213</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>344</td>\n",
       "      <td>351</td>\n",
       "      <td>346</td>\n",
       "      <td>477</td>\n",
       "      <td>473</td>\n",
       "      <td>481</td>\n",
       "      <td>355</td>\n",
       "      <td>349</td>\n",
       "      <td>355</td>\n",
       "      <td>242</td>\n",
       "      <td>241</td>\n",
       "      <td>242</td>\n",
       "      <td>167.34</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-01 00:04:00</td>\n",
       "      <td>213</td>\n",
       "      <td>211</td>\n",
       "      <td>211</td>\n",
       "      <td>343</td>\n",
       "      <td>350</td>\n",
       "      <td>346</td>\n",
       "      <td>478</td>\n",
       "      <td>473</td>\n",
       "      <td>482</td>\n",
       "      <td>358</td>\n",
       "      <td>349</td>\n",
       "      <td>355</td>\n",
       "      <td>243</td>\n",
       "      <td>241</td>\n",
       "      <td>242</td>\n",
       "      <td>163.04</td>\n",
       "      <td>9.22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            timestamp  T_data_1_1  T_data_1_2  T_data_1_3  \\\n",
       "0           0  2015-01-01 00:00:00         212         210         211   \n",
       "1           1  2015-01-01 00:01:00         212         211         211   \n",
       "2           2  2015-01-01 00:02:00         212         211         211   \n",
       "3           3  2015-01-01 00:03:00         213         211         211   \n",
       "4           4  2015-01-01 00:04:00         213         211         211   \n",
       "\n",
       "   T_data_2_1  T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  T_data_3_3  \\\n",
       "0         347         353         347         474         473         481   \n",
       "1         346         352         346         475         473         481   \n",
       "2         345         352         346         476         473         481   \n",
       "3         344         351         346         477         473         481   \n",
       "4         343         350         346         478         473         482   \n",
       "\n",
       "   T_data_4_1  T_data_4_2  T_data_4_3  T_data_5_1  T_data_5_2  T_data_5_3  \\\n",
       "0         346         348         355         241         241         243   \n",
       "1         349         348         355         241         241         243   \n",
       "2         352         349         355         242         241         242   \n",
       "3         355         349         355         242         241         242   \n",
       "4         358         349         355         243         241         242   \n",
       "\n",
       "   H_data  AH_data  \n",
       "0  167.85     9.22  \n",
       "1  162.51     9.22  \n",
       "2  164.99     9.22  \n",
       "3  167.34     9.22  \n",
       "4  163.04     9.22  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2015-01-04 00:05:00</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2015-01-04 01:05:00</td>\n",
       "      <td>384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2015-01-04 02:05:00</td>\n",
       "      <td>393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2015-01-04 03:05:00</td>\n",
       "      <td>399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2015-01-04 04:05:00</td>\n",
       "      <td>400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            timestamp  target\n",
       "0           0  2015-01-04 00:05:00     392\n",
       "1           1  2015-01-04 01:05:00     384\n",
       "2           2  2015-01-04 02:05:00     393\n",
       "3           3  2015-01-04 03:05:00     399\n",
       "4           4  2015-01-04 04:05:00     400"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По данному описанию качества продукции мы видим, что стандартной отклонение равно 46, запомним это значение в качестве отправной точки для baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    28000.000000\n",
       "mean       402.388857\n",
       "std         46.287060\n",
       "min        221.000000\n",
       "25%        372.000000\n",
       "50%        408.000000\n",
       "75%        438.000000\n",
       "max        505.000000\n",
       "Name: target, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['target'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для удобства сразу приведем данные о времени из типа string к типу datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train['timestamp'] = pd.to_datetime(x_train['timestamp'])\n",
    "y_train['timestamp'] = pd.to_datetime(y_train['timestamp'])\n",
    "x_test['timestamp'] = pd.to_datetime(x_test['timestamp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данные о качестве продукции начинаются с более позднего времени, чем данные с показателями работы оборудования, поэтому для удобства формирования тренировочного датасета обрежем данные с показателями работы обородувания до начала первого эксперимента, указанного в данных о качестве продукции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_delta(start_time: datetime, end_time: datetime) -> float:\n",
    "    \"\"\"Returns time difference in minutes.\n",
    "    \"\"\"\n",
    "    return (end_time - start_time).total_seconds() / 60 if start_time < end_time else -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_data_before_start(data: pd.Series, start_time: datetime) -> None:\n",
    "    \"\"\"Deletes non-actual by time data inplace.\n",
    "    \"\"\"\n",
    "    start = 0\n",
    "    while (get_time_delta(data['timestamp'].iloc[start], start_time) > 59):\n",
    "        start += 1\n",
    "\n",
    "    data.drop(data.index[0:start], inplace=True)\n",
    "    data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_data_before_start(x_train, y_train['timestamp'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве baseline рассмотрим следующую конструкцию, которая чуть более сложная, чем выводить просто среднее по таргетам для любого значения. \n",
    "\n",
    "А именно:\n",
    "\n",
    "    - выберем данные с показателями работы для каждого эксперимента (данные за час)\n",
    "    - посчитаем среднее по каждому столбцу для каждого такого набора\n",
    "    - по полученной строчке для эксперимента будем предсказывать значение качества продукции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_preprocessed_data(data: pd.Series, end_time: datetime) -> pd.Series:\n",
    "    \"\"\"Returns data aggregating mean value per hour.\n",
    "    \n",
    "    Args:\n",
    "        data: Source data.\n",
    "        end_time: Time of the last experiment.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe that contains preprocessed data from the corresponding input dataframe.\n",
    "    \"\"\"\n",
    "    cols = list(data)\n",
    "    cols.remove('Unnamed: 0')\n",
    "    \n",
    "    df = pd.DataFrame(columns=cols)\n",
    "    cols.remove('timestamp')\n",
    "    \n",
    "    for idx in tqdm(range(trunc(len(data) / 60))):\n",
    "        if (get_time_delta(data['timestamp'].iloc[idx], end_time) != -1):\n",
    "            row = data.loc[60*idx:60*idx+59, cols].mean()\n",
    "            row['timestamp'] = data['timestamp'].iloc[60*idx+59]\n",
    "            df.loc[idx] = row\n",
    "            \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████| 28000/28000 [03:31<00:00, 132.39it/s]\n"
     ]
    }
   ],
   "source": [
    "x_train_base_ = get_base_preprocessed_data(x_train, y_train['timestamp'].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_base = y_train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_base = x_train_base_.iloc[:, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural network preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве более сложного подхода, чем описанный ранее mean baseline, реализуем архитектуру нейронной сети для данной задачи.\n",
    "\n",
    "\n",
    "Для этого сформируем датасет, где каждый элемент будет представлять собой массив, состоящий из массива, размера $(59, 17)$, содержащий все данные об эксперименте и массива, состоящего из соотвествующего значения качества продукции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nn_preprocessed_data(data: pd.Series) -> pd.Series:\n",
    "    \"\"\"Returns data aggregating by experiment.\n",
    "    \n",
    "    Args:\n",
    "        data: Source data.\n",
    "        \n",
    "    Returns:\n",
    "        A dataframe that contains preprocessed data from the corresponding input dataframe.\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    data = data.iloc[:, 2:]\n",
    "\n",
    "    for idx in tqdm(range(trunc(len(data) / 60))):\n",
    "        X.append(data.iloc[60*idx:60*idx+59].to_numpy())\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████| 28000/28000 [00:03<00:00, 8857.09it/s]\n"
     ]
    }
   ],
   "source": [
    "X = get_nn_preprocessed_data(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = list(map(lambda x: [x], y_train['target']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(X) \n",
    "tensor_y = torch.Tensor(y)\n",
    "\n",
    "dataset = TensorDataset(tensor_x,tensor_y)\n",
    "\n",
    "train_size = int(0.9 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=256, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import xgboost as xgb\n",
    "\n",
    "%load_ext tensorboard\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве самого простого решения попробуем выводить среднее по таргетам для любого значения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_score = mean_absolute_error(y_train_base, [y_train_base.mean()]*len(y_train_base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['Mean'] = [mean_score]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для решения с датасетом из предобработанных средних для каждого эксперимента возьмем простую линейную модель и xgboost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_results(model, params={}):\n",
    "    name = type(model.named_steps['model']).__name__\n",
    "    model = GridSearchCV(model, param_grid=params, scoring='neg_mean_absolute_error', cv=3, verbose=1, n_jobs=-1)\n",
    "    model.fit(x_train_base, y_train_base)\n",
    "    results[name]=[-model.best_score_]\n",
    "    print(results[name])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[14.009556091343208]\n"
     ]
    }
   ],
   "source": [
    "lr = Pipeline(steps=[('normalize', MinMaxScaler()), ('model', LinearRegression())])\n",
    "get_model_results(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[9.070946138424569]\n"
     ]
    }
   ],
   "source": [
    "get_model_results(Pipeline(steps=[('normalize', MinMaxScaler()), ('model', xgb.XGBRegressor())]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуем несложную архитектуру нейронной сети. \\\n",
    "В качестве функции потерь для тренировки будем использовать MSE, так как она дифференцируема."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        def block(in_feat, out_feat, normalize=True):\n",
    "            layers = [nn.Linear(in_feat, out_feat)]\n",
    "            if normalize:\n",
    "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.pred = nn.Sequential(\n",
    "            *block(1003, 512),\n",
    "            *block(512, 256),\n",
    "            *block(256, 16),\n",
    "            torch.nn.Linear(16, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        return self.pred(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-07-21 15:10:07.435943: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-07-21 15:10:07.435990: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 48/48 [01:57<00:00,  2.45s/it]\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "loss_func = torch.nn.MSELoss()\n",
    "l1_loss = torch.nn.L1Loss()\n",
    "\n",
    "writer = SummaryWriter()\n",
    "\n",
    "for epoch in tqdm(range(48)):\n",
    "    for step, (batch_x, batch_y) in enumerate(train_dataloader): \n",
    "        \n",
    "        b_x = Variable(batch_x)\n",
    "        b_y = Variable(batch_y)\n",
    "\n",
    "        prediction = net(b_x)     \n",
    "\n",
    "        loss = loss_func(prediction, b_y)  \n",
    "        writer.add_scalar('MSE/train', loss, step)\n",
    "        writer.add_scalar('MAE/train', l1_loss(prediction, b_y), step)\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        loss.backward()         \n",
    "        optimizer.step()    \n",
    "    \n",
    "    test_loss = 0 \n",
    "    with torch.no_grad():\n",
    "        for data in test_dataloader:\n",
    "            x, y = data\n",
    "            pred = net(x)\n",
    "            test_loss += l1_loss(pred, y)\n",
    "    test_loss /= len(test_dataloader)\n",
    "    writer.add_scalar('MAE/test', test_loss, epoch)\n",
    "\n",
    "torch.save(net.state_dict(), 'net.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8080 (pid 37555), started 5:49:22 ago. (Use '!kill 37555' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-da21d2984d23c2eb\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-da21d2984d23c2eb\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8080;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tensorboard --logdir=runs --host=localhost --port=8080"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['neural_net'] = [test_loss.numpy()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуализируем полученные результаты с помощью таблички, в которой указан метод и его результаты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results():\n",
    "    sorted_res = dict(sorted(results.items(), key=lambda item: item[1][0]))\n",
    "    v = list(sorted_res.values())\n",
    "    fst = list(map(lambda x: x[0], v))\n",
    "    df = pd.DataFrame({\"name\": list(sorted_res.keys()), \"mae\": fst})\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None, 'display.max_colwidth', None):\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural_net</td>\n",
       "      <td>7.9916077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBRegressor</td>\n",
       "      <td>9.070946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LinearRegression</td>\n",
       "      <td>14.009556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mean</td>\n",
       "      <td>37.763177</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               name        mae\n",
       "0        neural_net  7.9916077\n",
       "1      XGBRegressor   9.070946\n",
       "2  LinearRegression  14.009556\n",
       "3              Mean  37.763177"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying best solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По результатам нейронная оказалась сеть наиболее удачным подходом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████| 1184/1184 [00:00<00:00, 9306.68it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 1184/1184 [00:08<00:00, 140.55it/s]\n"
     ]
    }
   ],
   "source": [
    "x = get_nn_preprocessed_data(x_test)\n",
    "tx = torch.Tensor(x) \n",
    "pred = net(tx)\n",
    "\n",
    "x_base = get_base_preprocessed_data(x_test, x_test['timestamp'].iloc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(columns=list(y_train))\n",
    "\n",
    "y_test['timestamp'] = x_base['timestamp']\n",
    "y_test['target'] = np.concatenate(pred.detach().numpy())\n",
    "y_test['Unnamed: 0'] = y_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.to_csv('y_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Software Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Напишите класс JsonTransformer для обработки входящих данных в предоставленном файле .json, приведения их к требуемому плоскому формату и сохранению в PostgreSQL\n",
    "\n",
    "Путь к файлу json -> JsonTransformer -> flat table @ PostgreSQL\n",
    "\n",
    "Вход - input_example.json\n",
    "Выход - плоская структура для записи в psql:\n",
    "\n",
    "`` \n",
    "[{\n",
    " data_id:\"266\",\n",
    " data_name:\"v.2\",\n",
    " data_sellerId:\"355\",\n",
    " statuses_id:\"1766\",\n",
    " statuses_name:\"КС: 10%\",\n",
    " statuses_amount\"100\",\n",
    " statuses_actionBaseId:\"266\"},\n",
    " ...]\n",
    "``\n",
    "\n",
    "(1 строка = 1 элемент внутри ^statuses, statuses_id как уникальный ключ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/furiousteabag/Projects/envs/base/lib/python3.7/site-packages/psycopg2/__init__.py:144: UserWarning: The psycopg2 wheel package will be renamed from release 2.8; in order to keep installing from binary please use \"pip install psycopg2-binary\" instead. For details see: <http://initd.org/psycopg/docs/install.html#binary-install-from-pypi>.\n",
      "  \"\"\")\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import psycopg2\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"скелет\" требуемого класса на Python\n",
    "import abc\n",
    "import json\n",
    "\n",
    "\n",
    "class BaseJsonTransformer(abc.ABC):\n",
    "    def __init__(self, json_file_path):\n",
    "        self.json_file_path = json_file_path\n",
    "\n",
    "    def read_json(self):\n",
    "        with open(self.json_file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def transform_json(self, json_to_transform=None):\n",
    "        return\n",
    "\n",
    "    @staticmethod\n",
    "    def write_to_psql(json, psql_address):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JsonTransformer(BaseJsonTransformer):\n",
    "    \n",
    "    @staticmethod\n",
    "    def traverse(name, d):\n",
    "        \"\"\"Traverses dict into flat structure for psql\"\"\"\n",
    "        res = [{}]\n",
    "        for k, v in d.items():\n",
    "            if (k[0] == '^'):\n",
    "                k = k[1:]\n",
    "            if isinstance(v, list) and len(v) > 0:\n",
    "                res_ = []\n",
    "                for vdict in v:\n",
    "                    res_.append(JsonTransformer.traverse(k, vdict))\n",
    "                res_ = np.concatenate(res_)\n",
    "\n",
    "                nres = []\n",
    "                for i in res:\n",
    "                    for j in res_:\n",
    "                        e = i\n",
    "                        e.update(j)\n",
    "                        nres.append(e)     \n",
    "                res = nres\n",
    "            elif isinstance(v, dict) and len(v) > 0:\n",
    "                res = JsonTransformer.traverse(k, v)\n",
    "            else:\n",
    "                kname = name + '_' + k if name else k\n",
    "                for rdict in res:\n",
    "                    rdict[kname] = v if v else 'null'\n",
    "        return res\n",
    "    \n",
    "    def transform_json(self, json_to_transform=None):\n",
    "        \"\"\"Transforms json into flat table for psql.\"\"\"\n",
    "        res = []\n",
    "        json = json_to_transform if json_to_transform else self.read_json()\n",
    "\n",
    "        for j in json:\n",
    "            res += JsonTransformer.traverse('', j)\n",
    "        return res\n",
    "    \n",
    "    @staticmethod\n",
    "    def write_to_psql(json, psql_address=\"host=localhost dbname=postgres user=postgres password=root\"):\n",
    "        \"\"\"Writes json in table at the specified address.\"\"\"\n",
    "        conn = psycopg2.connect(psql_address)\n",
    "        cur = conn.cursor()\n",
    "        #cur.execute(\"drop table if exists data\")\n",
    "        \n",
    "        j = json\n",
    "        cur.execute(\"create table if not exists data({})\".format(reduce(lambda x, y: x + ' text, ' + y, j[0].keys()) + ' text'))\n",
    "\n",
    "        for j_ in j:\n",
    "            query = \"insert into data values ({})\".format(\"\\'\" + reduce(lambda x, y: x + \"\\'\"  + \" , \" + \"\\'\"  + y, j[0].values()) +  \"\\'\")\n",
    "            cur.execute(query)\n",
    "        \n",
    "        conn.commit()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = JsonTransformer('input_example.json')\n",
    "j = trans.transform_json()\n",
    "trans.write_to_psql(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Data Engineering \\ Архитектура"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Ситуация*:\n",
    "- На удалённом сервере есть папка с csv файлами. \n",
    "- 1 раз в час в папку поступает новый csv файл (с данными за последний час). \n",
    "- В названии каждого файла содержится timestamp, указывающий время обновления файла.\n",
    "- Файл может содержать ряд неточностей, поэтому каждый новый файл требует обработки.\n",
    "\n",
    "*Задача*:\n",
    "- при появлении нового csv файла - в течение 60 секунд запускать его обработку\n",
    "- по итогу обработки - складывать получившийся результат в PostgreSQL\n",
    "- весь процесс - мониторить своевременность и успешность обработок\n",
    "\n",
    "\n",
    "##### Опишите архитектуру сбора и обработки для решения поставленной задачи:\n",
    "- 150-200 слов - достаточно\n",
    "- описание конкретных инструментов - приветствуется\n",
    "- при желании - можно описать всё схемой с минимальными комментариями "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение:\n",
    "\n",
    "- каждые 60 секунд запускается скрипт с помощью cron, который проверяет, нет ли csv-файла, который был получен в последние 60 секунд;\n",
    "    - проверка на наличие осуществляется с помощью вычисления разности между временем запуска скрипта и временем из названия файла;\n",
    "- если такой файл был найден, то:\n",
    "    - обрабатываем файл:\n",
    "        - проверяем на валидность;\n",
    "        - записываем в PostgreSQL;\n",
    "    - мониторинг осуществляем с помощью логгинга в Prometeus и Grafana;"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
